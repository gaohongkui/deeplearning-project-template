# Deep Learning Project Template

ä¸€ä¸ªåŸºäºPyTorchå’ŒHydraçš„æ·±åº¦å­¦ä¹ é¡¹ç›®æ¨¡æ¿ï¼Œä¸“æ³¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚æœ¬é¡¹ç›®æä¾›äº†å®Œæ•´çš„é¡¹ç›®ç»“æ„ã€é…ç½®ç®¡ç†å’Œè®­ç»ƒæµç¨‹ï¼Œä¾¿äºå¿«é€Ÿå¼€å§‹æ·±åº¦å­¦ä¹ é¡¹ç›®å¼€å‘ã€‚

## é¡¹ç›®ç‰¹æ€§

- ğŸš€ **æ¨¡å—åŒ–è®¾è®¡**: æ¸…æ™°çš„æ¨¡å—åˆ†ç¦»ï¼ˆæ¨¡å‹ã€æ•°æ®ã€è®­ç»ƒå™¨ã€é…ç½®ï¼‰
- âš™ï¸ **é…ç½®ç®¡ç†**: ä½¿ç”¨Hydraè¿›è¡Œçµæ´»çš„é…ç½®ç®¡ç†
- ğŸ“Š **å®éªŒè¿½è¸ª**: é›†æˆWeights & Biasesè¿›è¡Œå®éªŒè¿½è¸ª
- ğŸ”§ **ç±»å‹å®‰å…¨**: ä½¿ç”¨dataclassè¿›è¡Œç±»å‹å®‰å…¨çš„é…ç½®ç®¡ç†
- ğŸ“ˆ **è®­ç»ƒç›‘æ§**: å®Œæ•´çš„è®­ç»ƒæ—¥å¿—å’Œè¯„ä¼°æŒ‡æ ‡
- ğŸ§ª **å¯æ‰©å±•æ€§**: æ˜“äºæ‰©å±•æ–°çš„æ¨¡å‹ã€æ•°æ®é›†å’Œè®­ç»ƒç­–ç•¥

## é¡¹ç›®ç»“æ„

```
deeplearning-project-template/
â”œâ”€â”€ configs/                    # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ config.yaml            # ä¸»é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ data/                  # æ•°æ®é…ç½®
â”‚   â”‚   â””â”€â”€ glue.yaml          # GLUEæ•°æ®é›†é…ç½®
â”‚   â”œâ”€â”€ model/                 # æ¨¡å‹é…ç½®
â”‚   â”‚   â””â”€â”€ bert.yaml          # BERTæ¨¡å‹é…ç½®
â”‚   â””â”€â”€ training/              # è®­ç»ƒé…ç½®
â”‚       â””â”€â”€ base.yaml          # åŸºç¡€è®­ç»ƒé…ç½®
â”œâ”€â”€ data/                      # æ•°æ®ç›®å½•
â”‚   â””â”€â”€ glue/                  # GLUEæ•°æ®é›†
â”‚       â”œâ”€â”€ train.jsonl        # è®­ç»ƒæ•°æ®
â”‚       â””â”€â”€ val.jsonl          # éªŒè¯æ•°æ®
â”œâ”€â”€ src/                       # æºä»£ç ç›®å½•
â”‚   â””â”€â”€ deeplearning_project_template/
â”‚       â”œâ”€â”€ config.py          # é…ç½®ç±»å®šä¹‰
â”‚       â”œâ”€â”€ data.py            # æ•°æ®å¤„ç†æ¨¡å—
â”‚       â”œâ”€â”€ model.py           # æ¨¡å‹å®šä¹‰
â”‚       â”œâ”€â”€ trainer.py         # è®­ç»ƒå™¨
â”‚       â””â”€â”€ utils.py           # å·¥å…·å‡½æ•°ï¼ˆåŒ…å«æ¨¡å‹åŠ è½½ï¼‰
â”œâ”€â”€ train.py                   # è®­ç»ƒå…¥å£è„šæœ¬
â”œâ”€â”€ eval.py                    # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ predict.py                 # æ¨ç†è„šæœ¬
â”œâ”€â”€ pyproject.toml             # é¡¹ç›®ä¾èµ–é…ç½®
â””â”€â”€ uv.lock                    # ä¾èµ–é”å®šæ–‡ä»¶
```

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒé…ç½®

1. **å®‰è£…ä¾èµ–**:
   ```bash
   # ä½¿ç”¨uvåŒ…ç®¡ç†å™¨ï¼ˆæ¨èï¼‰
   uv sync
   
   # æˆ–è€…ä½¿ç”¨pip
   pip install -e .
   ```

2. **å®‰è£…å¼€å‘ä¾èµ–**:
   ```bash
   uv sync --group dev
   ```

### æ•°æ®å‡†å¤‡

é¡¹ç›®ä½¿ç”¨GLUEæ ¼å¼çš„æ•°æ®é›†ã€‚æ•°æ®æ–‡ä»¶åº”ä¸ºJSONLæ ¼å¼ï¼Œæ¯è¡ŒåŒ…å«ï¼š

```json
{"text": "Your text here", "label": 0}
```

å°†è®­ç»ƒæ•°æ®æ”¾åœ¨ `data/glue/train.jsonl`ï¼ŒéªŒè¯æ•°æ®æ”¾åœ¨ `data/glue/val.jsonl`ã€‚

### è®­ç»ƒæ¨¡å‹

è¿è¡Œè®­ç»ƒè„šæœ¬ï¼š

```bash
python train.py
```

ä½¿ç”¨è‡ªå®šä¹‰é…ç½®ï¼š

```bash
# ä½¿ç”¨ä¸åŒçš„æ¨¡å‹é…ç½®
python train.py model=bert

# ä½¿ç”¨ä¸åŒçš„è®­ç»ƒé…ç½®
python train.py training=base

# è¦†ç›–ç‰¹å®šå‚æ•°
python train.py training.learning_rate=1e-5 training.num_train_epochs=5

# å¤šé…ç½®ç»„åˆ
python train.py model=bert data=glue training=base
```

### è¯„ä¼°æ¨¡å‹

ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼ˆæ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶ï¼‰ï¼š

**æ–¹å¼1ï¼šä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°**
```bash
# è¯„ä¼°æœ€ä½³æ¨¡å‹
python eval.py \
  --checkpoint outputs/2025-10-31/01-10-48/outputs/best_model \
  --test_file data/glue/val.jsonl

# è¯„ä¼°ç‰¹å®šæ£€æŸ¥ç‚¹ï¼Œè‡ªå®šä¹‰å‚æ•°
python eval.py \
  --checkpoint outputs/2025-10-31/01-10-48/outputs/checkpoint-2 \
  --test_file data/glue/test.jsonl \
  --batch_size 16 \
  --max_seq_length 512 \
  --device cpu

# ä¿å­˜é¢„æµ‹ç»“æœ
python eval.py \
  --checkpoint outputs/.../best_model \
  --test_file data/glue/test.jsonl \
  --save_predictions
```

**æ–¹å¼2ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶**
```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆéœ€å…ˆç¼–è¾‘ configs/eval.yamlï¼‰
python eval.py --config configs/eval.yaml

# æ³¨æ„ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶æ—¶ï¼Œæ‰€æœ‰å‚æ•°ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œå‘½ä»¤è¡Œå‚æ•°å°†è¢«å¿½ç•¥
```

å‚æ•°è¯´æ˜ï¼š
- `--checkpoint`: æ£€æŸ¥ç‚¹ç›®å½•è·¯å¾„ï¼ˆå¿…éœ€ï¼Œtokenizer ä¼šè‡ªåŠ¨ä»æ£€æŸ¥ç‚¹åŠ è½½ï¼‰
- `--test_file`: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- `--config`: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼ŒYAMLæ ¼å¼ï¼‰
- `--batch_size`: æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ï¼š32ï¼‰
- `--max_seq_length`: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ï¼š512ï¼‰
- `--device`: è®¾å¤‡ç±»å‹ï¼ˆé»˜è®¤ï¼šcudaï¼‰
- `--save_predictions`: æ˜¯å¦ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ–‡ä»¶

### æ¨¡å‹æ¨ç†

å¯¹æ–°æ–‡æœ¬è¿›è¡Œé¢„æµ‹ï¼š

**æ–¹å¼1ï¼šä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°**
```bash
# é¢„æµ‹å•ä¸ªæ–‡æœ¬
python predict.py \
  --checkpoint outputs/2025-10-31/01-10-48/outputs/best_model \
  --text "This is a great movie!"

# æ‰¹é‡é¢„æµ‹ï¼ˆä»æ–‡ä»¶è¯»å–ï¼Œæ¯è¡Œä¸€ä¸ªæ–‡æœ¬ï¼‰
python predict.py \
  --checkpoint outputs/2025-10-31/01-10-48/outputs/best_model \
  --text_file texts.txt \
  --output_file predictions.json

# è‡ªå®šä¹‰å‚æ•°
python predict.py \
  --checkpoint outputs/.../best_model \
  --text "Your text here" \
  --tokenizer_name bert-base-uncased \
  --device cpu
```

**æ–¹å¼2ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶**
```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶
python predict.py --config configs/predict.yaml
```

å‚æ•°è¯´æ˜ï¼š
- `--checkpoint`: æ£€æŸ¥ç‚¹ç›®å½•è·¯å¾„ï¼ˆå¿…éœ€ï¼Œtokenizer ä¼šè‡ªåŠ¨ä»æ£€æŸ¥ç‚¹åŠ è½½ï¼‰
- `--text`: å•ä¸ªæ–‡æœ¬ï¼ˆä¸ --text_file äºŒé€‰ä¸€ï¼‰
- `--text_file`: æ‰¹é‡æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªæ–‡æœ¬ï¼‰
- `--config`: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
- `--max_seq_length`: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ï¼š512ï¼‰
- `--device`: è®¾å¤‡ç±»å‹ï¼ˆé»˜è®¤ï¼šcudaï¼‰
- `--output_file`: ä¿å­˜é¢„æµ‹ç»“æœçš„æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼ŒJSONæ ¼å¼ï¼‰

### é…ç½®è¯´æ˜

é¡¹ç›®ä½¿ç”¨Hydraè¿›è¡Œé…ç½®ç®¡ç†ï¼Œä¸»è¦é…ç½®ç±»åˆ«ï¼š

- **æ¨¡å‹é…ç½®** (`configs/model/`): å®šä¹‰æ¨¡å‹ç»“æ„å‚æ•°
- **æ•°æ®é…ç½®** (`configs/data/`): å®šä¹‰æ•°æ®å¤„ç†å‚æ•°
- **è®­ç»ƒé…ç½®** (`configs/training/`): å®šä¹‰è®­ç»ƒè¿‡ç¨‹å‚æ•°

## æ ¸å¿ƒæ¨¡å—

### 1. é…ç½®ç®¡ç† (`src/deeplearning_project_template/config.py`)

ä½¿ç”¨dataclasså®šä¹‰ç±»å‹å®‰å…¨çš„é…ç½®ç±»ï¼š

- `ModelConfig`: æ¨¡å‹ç»“æ„é…ç½®
- `DataConfig`: æ•°æ®å¤„ç†é…ç½®
- `TrainingConfig`: è®­ç»ƒè¿‡ç¨‹é…ç½®
- `Config`: é¡¶å±‚å®éªŒé…ç½®

### 2. æ¨¡å‹å®šä¹‰ (`src/deeplearning_project_template/model.py`)

å®ç°Transformeræ¨¡å‹æ¶æ„ï¼š

- `TransformerModel`: ä¸»æ¨¡å‹ç±»
- `TransformerLayer`: Transformerå±‚
- `MultiHeadAttention`: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶

### 3. æ•°æ®å¤„ç† (`src/deeplearning_project_template/data.py`)

æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼š

- `TextDataset`: æ–‡æœ¬åˆ†ç±»æ•°æ®é›†
- `DataModule`: æ•°æ®æ¨¡å—ï¼Œå°è£…æ•°æ®åŠ è½½é€»è¾‘

### 4. è®­ç»ƒå™¨ (`src/deeplearning_project_template/trainer.py`)

è®­ç»ƒæµç¨‹ç®¡ç†ï¼š

- æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯
- æŒ‡æ ‡è®¡ç®—å’Œæ—¥å¿—è®°å½•
- æ¨¡å‹ä¿å­˜å’Œæ—©åœ
- æ£€æŸ¥ç‚¹ç®¡ç†å’Œæœ€ä½³æ¨¡å‹ä¿å­˜

### 5. å·¥å…·å‡½æ•° (`src/deeplearning_project_template/utils.py`)

è¾…åŠ©å·¥å…·ï¼š

- `load_checkpoint()`: ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œè®­ç»ƒçŠ¶æ€

### 6. è¯„ä¼°è„šæœ¬ (`eval.py`)

ç‹¬ç«‹è¯„ä¼°è„šæœ¬ï¼š

- åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
- åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ€§èƒ½
- æ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶

### 7. æ¨ç†è„šæœ¬ (`predict.py`)

ç‹¬ç«‹æ¨ç†è„šæœ¬ï¼š

- å¯¹æ–°æ–‡æœ¬è¿›è¡Œé¢„æµ‹
- æ”¯æŒå•ä¸ªæ–‡æœ¬æˆ–æ‰¹é‡æ–‡æœ¬
- è¿”å›é¢„æµ‹æ ‡ç­¾å’Œç½®ä¿¡åº¦
- æ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶

## ä½¿ç”¨ç¤ºä¾‹

### åœ¨ Python ä»£ç ä¸­ä½¿ç”¨

åœ¨ Python ä»£ç ä¸­åŠ è½½å’Œä½¿ç”¨æ¨¡å‹ï¼š

```python
from deeplearning_project_template.utils import load_checkpoint
import torch

# åŠ è½½æ¨¡å‹å’Œ tokenizerï¼ˆtokenizer ä¼šä»æ£€æŸ¥ç‚¹è‡ªåŠ¨åŠ è½½ï¼‰
model, tokenizer, _, state = load_checkpoint("outputs/.../best_model", device="cuda")

# é¢„æµ‹
text = "This is a test sentence."
encoding = tokenizer(text, max_length=512, padding="max_length", 
                     truncation=True, return_tensors="pt")
input_ids = encoding["input_ids"].to("cuda")
attention_mask = encoding["attention_mask"].to("cuda")

model.eval()
with torch.no_grad():
    logits = model(input_ids, attention_mask)
    prediction = torch.argmax(logits, dim=-1).item()
    
print(f"Predicted label: {prediction}")
print(f"Best metric from training: {state['best_metric']:.4f}")
```

### é…ç½®è¯´æ˜

é¡¹ç›®ä½¿ç”¨Hydraè¿›è¡Œé…ç½®ç®¡ç†ï¼Œä¸»è¦é…ç½®ç±»åˆ«ï¼š

- **æ¨¡å‹é…ç½®** (`configs/model/`): å®šä¹‰æ¨¡å‹ç»“æ„å‚æ•°
- **æ•°æ®é…ç½®** (`configs/data/`): å®šä¹‰æ•°æ®å¤„ç†å‚æ•°
- **è®­ç»ƒé…ç½®** (`configs/training/`): å®šä¹‰è®­ç»ƒè¿‡ç¨‹å‚æ•°

## æ ¸å¿ƒæ¨¡å—

### 1. é…ç½®ç®¡ç† (`src/deeplearning_project_template/config.py`)

ä½¿ç”¨dataclasså®šä¹‰ç±»å‹å®‰å…¨çš„é…ç½®ç±»ï¼š

- `ModelConfig`: æ¨¡å‹ç»“æ„é…ç½®
- `DataConfig`: æ•°æ®å¤„ç†é…ç½®
- `TrainingConfig`: è®­ç»ƒè¿‡ç¨‹é…ç½®
- `Config`: é¡¶å±‚å®éªŒé…ç½®

### 2. æ¨¡å‹å®šä¹‰ (`src/deeplearning_project_template/model.py`)

å®ç°Transformeræ¨¡å‹æ¶æ„ï¼š

- `TransformerModel`: ä¸»æ¨¡å‹ç±»
- `TransformerLayer`: Transformerå±‚
- `MultiHeadAttention`: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶

### 3. æ•°æ®å¤„ç† (`src/deeplearning_project_template/data.py`)

æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼š

- `TextDataset`: æ–‡æœ¬åˆ†ç±»æ•°æ®é›†
- `DataModule`: æ•°æ®æ¨¡å—ï¼Œå°è£…æ•°æ®åŠ è½½é€»è¾‘

### 4. è®­ç»ƒå™¨ (`src/deeplearning_project_template/trainer.py`)

è®­ç»ƒæµç¨‹ç®¡ç†ï¼š

- æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯
- æŒ‡æ ‡è®¡ç®—å’Œæ—¥å¿—è®°å½•
- æ¨¡å‹ä¿å­˜å’Œæ—©åœ
- æ£€æŸ¥ç‚¹ç®¡ç†å’Œæœ€ä½³æ¨¡å‹ä¿å­˜

### 5. å·¥å…·å‡½æ•° (`src/deeplearning_project_template/utils.py`)

è¾…åŠ©å·¥å…·ï¼š

- `load_checkpoint()`: ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œè®­ç»ƒçŠ¶æ€

### 6. è¯„ä¼°è„šæœ¬ (`eval.py`)

ç‹¬ç«‹è¯„ä¼°è„šæœ¬ï¼š

- åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
- åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ€§èƒ½
- æ”¯æŒå‘½ä»¤è¡Œå‚æ•°é…ç½®

## é…ç½®ç¤ºä¾‹

### åŸºç¡€é…ç½® (`configs/config.yaml`)

```yaml
defaults:
  - model: bert
  - data: glue
  - training: base
  - _self_

experiment_name: text_classification
description: "Text classification experiment"
tags:
  - nlp
  - classification

device: cuda
distributed: false
```

### BERTæ¨¡å‹é…ç½® (`configs/model/bert.yaml`)

```yaml
model_type: bert
tokenizer_name: bert-base-uncased  # ä¸æ¨¡å‹é…å¥—çš„ tokenizer

# æ¨¡å‹ç»“æ„å‚æ•°
hidden_size: 768
num_hidden_layers: 12
num_attention_heads: 12
intermediate_size: 3072

# Dropout å‚æ•°
hidden_dropout_prob: 0.1
attention_probs_dropout_prob: 0.1

# å…¶ä»–å‚æ•°
max_position_embeddings: 512
vocab_size: 30522  # å¿…é¡»ä¸ tokenizer çš„ vocab å¤§å°åŒ¹é…
```

### è®­ç»ƒé…ç½® (`configs/training/base.yaml`)

```yaml
output_dir: ${hydra:run.dir}/outputs
num_train_epochs: 3
learning_rate: 5e-5
weight_decay: 0.01
warmup_ratio: 0.1

optimizer_type: adamw
scheduler_type: linear

gradient_accumulation_steps: 1
max_grad_norm: 1.0

eval_steps: 500
save_steps: 500
save_total_limit: 3
logging_steps: 100

early_stopping_patience: 3
early_stopping_threshold: 0.001

seed: 42
```

## æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°æ¨¡å‹

1. åœ¨ `configs/model/` ä¸­åˆ›å»ºæ–°çš„é…ç½®æ–‡ä»¶

ä¾‹å¦‚ï¼Œæ·»åŠ  RoBERTa æ¨¡å‹ï¼š

```yaml
# configs/model/roberta.yaml
model_type: roberta
tokenizer_name: roberta-base  # æ¨¡å‹é…å¥—çš„ tokenizer

hidden_size: 768
num_hidden_layers: 12
num_attention_heads: 12
intermediate_size: 3072
vocab_size: 50265  # å¿…é¡»ä¸ tokenizer çš„ vocab å¤§å°åŒ¹é…
```

2. ä½¿ç”¨æ–°æ¨¡å‹è®­ç»ƒï¼š

```bash
python train.py model=roberta
```

**æ³¨æ„**: tokenizer ä¸æ¨¡å‹å¼ºç»‘å®šï¼Œåˆ‡æ¢æ¨¡å‹é…ç½®ä¼šè‡ªåŠ¨åˆ‡æ¢å¯¹åº”çš„ tokenizer

### æ·»åŠ æ–°æ•°æ®é›†

1. åœ¨ `src/deeplearning_project_template/data.py` ä¸­æ·»åŠ æ–°çš„Datasetç±»
2. åœ¨ `configs/data/` ä¸­åˆ›å»ºå¯¹åº”çš„é…ç½®æ–‡ä»¶
3. æ›´æ–° `DataConfig` ç±»ä»¥æ”¯æŒæ–°æ•°æ®é›†çš„å‚æ•°

### è‡ªå®šä¹‰è®­ç»ƒç­–ç•¥

1. ä¿®æ”¹ `src/deeplearning_project_template/trainer.py` ä¸­çš„è®­ç»ƒé€»è¾‘
2. åœ¨ `configs/training/` ä¸­åˆ›å»ºæ–°çš„è®­ç»ƒé…ç½®æ–‡ä»¶
3. æ›´æ–° `TrainingConfig` ç±»ä»¥æ”¯æŒæ–°çš„è®­ç»ƒå‚æ•°

## ä¾èµ–é¡¹

ä¸»è¦ä¾èµ–ï¼š

- `torch>=2.0.0`: PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
- `transformers>=4.35.0`: Hugging Face Transformersåº“
- `hydra-core>=1.3.0`: é…ç½®ç®¡ç†æ¡†æ¶
- `omegaconf>=2.3.0`: é…ç½®å¯¹è±¡ç®¡ç†
- `wandb>=0.16.0`: å®éªŒè¿½è¸ª
- `scikit-learn>=1.3.0`: æœºå™¨å­¦ä¹ å·¥å…·

å¼€å‘ä¾èµ–ï¼š

- `notebook>=7.4.7`: Jupyterç¬”è®°æœ¬
- `ruff>=0.14.2`: ä»£ç æ ¼å¼åŒ–

## è´¡çŒ®æŒ‡å—

1. Fork æœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## æ›´æ–°æ—¥å¿—

### v0.1.0 (2025-10-29)
- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒBERTæ¨¡å‹å’Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡
- é›†æˆHydraé…ç½®ç®¡ç†
- æ·»åŠ Weights & Biaseså®éªŒè¿½è¸ª